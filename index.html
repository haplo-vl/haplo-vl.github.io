<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>HaploVL: A Single-Transformer Baseline for Multi-Modal Understandinge</title>
  <link rel="icon" type="image/x-icon" href="static/images/bot.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yangr116.github.io/" target="_blank" style="color:#50b08f !important">Rui Yang</a>,</span>
                <span class="author-block">
                  <a href="https://linsong.info/" target="_blank">Lin Song</a><sup>*</sup>,</span>
                  <span class="author-block"><a href="https://scholar.google.com/citations?user=oakZP0cAAAAJ&hl=en" target="_blank" style="color:#b249f8 !important">Yicheng Xiao</a>,</span>
                  <span class="author-block"><a href="https://scholar.google.com/citations?user=B5zcj4wAAAAJ&hl=zh-CN" target="_blank" style="color:#50b08f !important">Runhui Huang</a>,</span>
                  <span class="author-block"><a href="https://geyixiao.com/" target="_blank">Yixiao Ge</a>,</span>
                  <span class="author-block"><a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en" target="_blank">Ying Shan</a>,</span>
                  <span class="author-block"><a href="https://hszhao.github.io/" target="_blank" style="color:#50b08f !important">Hengshuang Zhao</a><sup>*</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><b style="color:#50b08f; font-weight:normal">&#x25B6 </b> The University of Hong Kong</b></span>
                    <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> ARC Lab, Tencent</span>
                    <span class="author-block"><b style="color:#b249f8; font-weight:normal">&#x25B6 </b> Tsinghua University</span>
                  </div>
                  
                  <div class="column has-text-centered">
                    <div class="publication-links">
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://github.com/Tencent/HaploVLM/blob/main/assets/HaploVL.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Tencent/HaploVLM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Huggingface Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/collections/stevengrove/haplo-67d2582ac79d96983fa99697" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ðŸ¤—
                  </span>
                  <span>Models</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in large language models (LLMs) have significantly propelled the development of large multi-modal models (LMMs), highlighting the potential for general and intelligent assistants. However, most LMMs model visual and textual modalities separately, leading to recent efforts to develop native LMMs using one transformer. Despite the promise, these native models are resource-intensive and often exhibit performance gaps compared to their compositional counterparts. To alleviate this issue, we propose a simple yet efficient method to construct a baseline for the native and end-to-end large multi-modal model in a single transformer. First, we propose a new multi-modal transformer model that can fuse multi-modal inputs in the early stage and respond to visual instructions in an auto-regressive manner. Second, we devise an efficient training recipe for the proposed model, which harnesses the prior knowledge of the pre-trained models, addressing both the performance limitations and the challenge of resource consumption. The proposed model demonstrates superior performance compared to other LMMs using one transformer and significantly narrows the performance gap with compositional LMMs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper method -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered"><img id="painting_icon" width="4%" src="https://cdn-icons-png.flaticon.com/512/1698/1698535.png"> Method</h2>

        <!-- Subtitle: Three Stage Training -->
        <h3 class="subtitle is-4" style="margin-top: 20px;">Model Architecture:</h3>
        <!-- Image section -->
        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/framework.png" alt="Method Illustration" style="max-width: 100%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Figure 1: The model architecture. A single transformer is used to process multi-modal sequences. Bidriectional mask is utilized for visual tokens and causal mask is utilized for textual tokens.
          </p>
        </div>
        <!-- Subtitle: Three Stage Training -->
        <h3 class="subtitle is-4" style="margin-top: 20px;">Training Receipe:</h3>

        <!-- Image section -->
        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/train-pipeline.png" alt="Method Illustration" style="max-width: 100%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Figure 2: The training pipeline of our HaploVL. During the pre-training stage (a), the pre-decoder is trained by distilling knowledge from the pre-trained vision encoder and the text embeddings of the LLM. Heads and teacher models are dropped after pre-training. In the full fine-tuning stage (b), the entire model is fine-tuned using visual instruction data.
          </p>
        </div>

        <!-- Additional text (optional) -->
        <!-- <div class="content has-text-justified" style="margin-top: 20px;">
          <p>
            The method involves several key steps, including data preprocessing, model training, and evaluation. 
            The above figure provides a visual overview of the process.
          </p> -->
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper method -->


<!-- Paper results -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/5886/5886212.png"> Results</h2>

        <!-- Subtitle: Main results -->
        <h3 class="subtitle is-4" style="margin-top: 20px;">Main results</h3>

        <!-- Image section -->
        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/results/main-results.png" alt="Main results" style="max-width: 100%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Table 1: Comparison on multi-modal benchmarks. `*' denotes images of related training datasets are observed during training. Haplo-8B-MI is the model further fine-tuned on multi-image datasets.
          </p>
        </div>

        <!-- Subtitle: Ablation study -->
        <h3 class="subtitle is-4" style="margin-top: 20px;">Ablation study</h3>

        <!-- Subtitle: ablation 1 -->
        <h3 class="subtitle is-5" style="margin-top: 20px;">Ablation for different LLMs, resolution, and visual instruction data:</h3>

        <!-- Image section -->
        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/results/ablation_study_1.png" alt="ablation study" style="max-width: 40%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Table 2: Ablation for different LLMs, resolution (Res.), and visual instruction data (Data-S3). `*' denotes that images from training datasets are used during training.
          </p>
        </div>

        <div class="content">
          <ul>ðŸŒŸ <span style="background-color: #e2f4ff;">A more advanced language model delivers significantly superior results.</span></ul>
          <ul>ðŸŒŸ <span style="background-color: #e2f4ff;">Higher input resolution enhances performance, as the LMM can capture finer-grained visual details.</span></ul>
          <ul>ðŸŒŸ <span style="background-color: #e2f4ff;">Expanding the visual instruction tuning data leads to substantial improvements by enriching the LMM's knowledge.</span></ul>
        </div>

        <!-- Subtitle: ablation 2 -->
        <h3 class="subtitle is-5" style="margin-top: 20px;">Ablation for pretraining stage (stage 1):</h3>

        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/results/ablation_study_2.png" alt="ablation study" style="max-width: 80%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Table 3: Ablation for pretraining stage.
          </p>
        </div>

        <div class="content">
          <ul>ðŸŒŸ <span style="background-color: #e2f4ff;">pretraining stage accelerates the convergence process.</span></ul>
        </div>

        <!-- Subtitle: ablation 3 -->
        <h3 class="subtitle is-5" style="margin-top: 20px;">Compared with the compositional LMM using the same LLM and training data:</h3>

        <!-- <div class="columns">
          <div class="column">
            <img src="static/images/results/ablation_3_1.png" alt="Image 1" style="width: 100%; height: auto;">
            <p class="is-italic" style="text-align: left; margin-top: 10px;">
              Table 4: Comparison with LLaVA-1.5-7B using the same LLM, resolution and 665K instruction data. `One Trans.' indicates whether the model belongs to one multi-modal transformer. All models depend on Vicuna-7B and 665K instruction data.
            </p>
          </div>
          <div class="column">
            <img src="static/images/results/ablation_3_2.png" alt="Image 2" style="width: 100%; height: auto;">
            <p class="is-italic" style="text-align: left; margin-top: 10px;">
              Table 5: Comparison with LLaVA-1.5-7B on MMStar. CP: coarse perception, FP: fine-grained perception, IR: instance reasoning, LR: logical reasoning, ST: science and technology, and MA: mathematics.
            </p>
          </div>
        </div> -->

        <!-- Image section -->
        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/results/ablation_study_3.png" alt="ablation study" style="max-width: 40%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Table 4: Comparison with LLaVA-1.5-7B on MMVP and MMStar. CP: coarse perception, FP: fine-grained perception, IR: instance reasoning, LR: logical reasoning, ST: science and technology, and MA: mathematics.
          </p>
        </div>


        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/results/case_study.jpg" alt="case study" style="max-width: 100%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Figure 3:Qualitative comparison of LLaVA-1.5-7B and our Haplo-7B. The first line involves cases about fine-grained perception. The second line includes cases of logical reasoning that depend on fine-grained perception.
          </p>
        </div>


        <div class="content">
          <ul>ðŸŒŸ <span style="background-color: #e2f4ff;">Early fusion of the textual and visual embeddings is beneficial for fine-grained perception.</span></ul>
        </div>


        <div class="has-text-centered" style="margin-top: 20px;">
          <img src="static/images/attn_map.png" alt="attn map" style="max-width: 100%; height: auto;">
          <p class="is-italic" style="text-align: left; margin-top: 10px;">
            Figure 4: Visualization for the early fusion mechanism of our single transformer. The second row illustrates the attention map of the gray words concerning the vision embeddings after the pre-decoder.
          </p>
        </div>

        <!-- Subtitle: Qualitative results -->
        <!-- <h3 class="subtitle is-4" style="margin-top: 20px;">Case study</h3> -->

        <!-- Additional text (optional) -->
        <!-- <div class="content has-text-justified" style="margin-top: 20px;">
          <p>
            The method involves several key steps, including data preprocessing, model training, and evaluation. 
            The above figure provides a visual overview of the process.
          </p>
        </div> -->
      </div>
    </div>
  </div>
</section>
<!-- End paper results -->


<!-- Paper case -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">
          <img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Examples
        </h2>
      </div>
    </div>

    <!-- Image carousel -->
    <div class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel" style="width: 100%; margin: 0 auto;">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/case/case_1.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                <!-- First image description. -->
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/case/case_2.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                <!-- Second image description. -->
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/case/case_3.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                <!-- Third image description. -->
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/case/case_4.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                <!-- Fourth image description. -->
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/case/case_5.png" alt="MY ALT TEXT"/>
              <h2 class="subtitle has-text-centered">
                <!-- Fourth image description. -->
              </h2>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End image carousel -->
  </div>
</section>
<!-- End paper case -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{yang2024haplo,
          title={HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding},
          author={Yang, Rui and Song, Lin and Xiao, Yicheng and Huang, Runhui and Ge, Yixiao and Shan, Ying and Zhao, Hengshuang},
          journal={arXiv preprint arXiv:xxxx.xxxxx},
          year={2025}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
